deep learning
uses neural networks
inspired by human brain
was shown on lecture slides, so i think they want us to use something like this

works by changing weights
forward propagation(uses weights to calculate outputs), back propagation(uses difference from expected and actual output to change weights and biasis)
Gradient Descent 
calculate loss using loss function

sigmoid or tanh functions used for activation function - 
	decides wether a neurone is activated or not

leaky reLU function, less computationally expensive

optimisers will change your model based on the output of the loss func

plain neural networks cannot handle sequential data(like sentences)
need a learning framework able to deal with variable length sequences, maintain sequence order,
share parameters across the sequence, keep track of long term dependancies
recurrent neural networks









on TSE:
split reviews up into sentences
	RNN and BTT
	LSTM/GRNN - work well with sentiment analysis









summary:
activation func will calculate how much neurones will contribute to next layer
(sigmoid, reLU)
loss func will calculate how wrong predicted outputs are compared to expected output
(squared error loss)
optimisers will adjust parameters(weights) to minimise loss func, making the model as optimized as possible
(gradient descent, SGD)

learning rate ensures that steps taken to change weight are not too large
model parameter - values learned from data and define skill of model
(weights)
model hyperparameter - used in processes to estimate model parameters
(learning rate, epochs)

epoch - when the entire data set is passed forward and backward through the neural network, only once
(need to read song lyrics more than once to learn them)
batches - data set split into batches when it is too large to pass into model all at once
itererations - number of batches needed to complete one epoch

make sure overfitting does not occur - where model becomes too specialised to the dataset

validation testing - cross validation stops overfitting

give reasons for why we split the data the way we do

dropout - can be used during optimisation

